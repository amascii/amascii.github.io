---
layout: post
title:  "3 ICASSP Papers I Found Interesting"
author: Armando Sierra
date:   2020-05-07 20:00:00 +0900
categories: academics
tags: icassp ieee graph-signals machine-learning

---

Because of the coronavirus epidemic,
the 45th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), to be held in Barcelona, 
went completely virtual. Also the usual participation fee was waived, 
making it possible for anyone to participate. 
Being a poor sucker for free things, I of course decided to 
join the fun.<sup>[1](#myfootnote1)</sup>

Here I'd like to talk about 3 papers that I found particularly interesting.
Two of them concern the current signals processing trend of *graph signals*.
One deals with the down-sampling of point clouds (via graph representation)
, while the other deals with denoising graphs.
The last paper is musical in nature;
it tackles the problem of cover song identification.

#### 1. [LEARNING LOCAL STRUCTURE OF REPRESENTATIVE POINTS FOR POINT CLOUD CLASSIFICATION AND SEMANTIC SEGMENTATION](https://ieeexplore.ieee.org/document/9054174)
The goal here is to reduce the number of points in a
[point cloud](https://en.wikipedia.org/wiki/Point_cloud), 
conserving only the most meaningful ones (*representative points*).
The benefits of using less to represent the same are clear:
less calculations are needed when processing.

Of course, there's nothing new about down-sampling.
The problem is, existing methods, particularly Farthest Point Sampling
(FPS) only considers the geometric information of points (as the name
implies). The author proposes a learning-based method to
extract the most representative points. 

The metric used here is the
classification accuracy on the 
[ModelNet40 dataset](https://modelnet.cs.princeton.edu/).
The proposed method achieves competitive state-of-the-art performance.
Not the best in terms of raw accuracy, but perhaps the best 
when considering its computational complexity.
In other words, it classifies just as well with less.


#### 2. [GRAPH AUTO-ENCODER FOR GRAPH SIGNAL DENOISING](https://ieeexplore.ieee.org/document/9053623)
Noise is pretty much always something we want to get rid off.
It distorts signals and makes it harder to extract meaningful information.
This paper talks about getting rid of noise in graph signals.

Again, denoising isn't a new concept, even for graph signals.
The difference here is 
1. the use of a 
[graph convolutional network](https://arxiv.org/abs/1609.02907) 
(GCN), and
2. the use of Kron-reduction-based pooling in said network

for the purpose of denoising.

The main innovation here is the use of Kron reduction for pooling.
In a few words, Kron reduction consists in taking all the `N` nodes of a 
graph, ordering them in some way (by degree, for example) and saving 
`Nt` of them (or removing `Ns = N - Nt` of them). Then, in order to
calculate the reduced graph's edges, we multiply some Laplacian matrices,
belonging to a combination of the `Nt` and `Ns` nodes. (Easier than it
sounds, really. See the paper's section on Kron reduction.)
My mentor remarked that this step is probably computationally intensive.

The authors test their denoising method on the 
[Seattle Loop dataset](https://github.com/zhiyongc/Seattle-Loop-Data).
Compared to other methods (Gaussian filter, Graph spectral filter), 
it achieves the least error in the reconstructed signal. 
Also, the GCN with Kron-reduction-based pooling constantly outperforms the 
plain ol' GCN, thereby proving the effectiveness of that long-named pooling.

#### 3. [SIMILARITY LEARNING FOR COVER SONG IDENTIFICATION USING CROSS-SIMILARITY MATRICES OF MULTI-LEVEL DEEP SEQUENCES](https://ieeexplore.ieee.org/document/9053257)
Have you ever heard a song, 
thought "hey, that sounds cool, I should Shazam it", 
proceeded to Shazam it, only to be let down because it was a cover song?
Maybe once or twice.

This paper focuses on the problem of Cover Song Identification, which
problem is pretty self explanatory.
Up until now, identification was done by extracting a song's
feature vector of fixed-length. Now this length-being-fixed thing is a 
problem since it ignores temporal information (pretty important
in music!). So the author proposed this cool-looking Siamese 
Convolutional Neural Network (SCNN) which takes the original song and 
the supposed cover as input. It does some funky stuff in the middle
and outputs the similarity of the two songs.

Now the paper talks about CQ-sepctrograms and multi-level 
deep representation, which is admittedly all very interesting,
but I'd like to draw your attention to this:

<p align="center">
  <img src="/assets/icassp2020-cross-similarity-matrix.jpg" alt="icassp2020-cross-similarity-matrix"/>
</p>

What you see here is the visualization of the cross-similarity matrices
of 
1. Up above, the original song and its cover
2. Down below, two non-related songs

We can see that a song and its cover produce a matrix with
many diagonals (most visible in the rightmost representation).
Non-related songs, on the other hand, show a plaid pattern.
Isn't that frikkin' cool? 
Feeding these cross-similarity matrices into a CNN, 
we can estimate the similarity of the input.

As for the experimental results,
let's just say they obliterated the competition. 
They achieved the best results across many datasets (with different genres),
which is a good sign of the model's generalization ability.

<sup><a name="myfootnote1">1</a></sup> In truth, our lab advisor had us watch ICASSP 
and report on 3 papers we found interesting. 
